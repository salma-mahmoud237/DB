{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF26L8QV/Bo99YwJuAF2L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salma-mahmoud237/DB/blob/main/Assignment_sheet4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TKrumB_3NKg",
        "outputId": "baab1853-d57d-492b-f133-5c2f256e524f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and Evaluate Models\n",
            "\n",
            " Model Performance Comparison:\n",
            "| Model                    | Train Accuracy | Test Accuracy |\n",
            "| Decision Tree            | 1.0000       | 0.9474        |\n",
            "| Random Forest            | 1.0000       | 0.9649        |\n",
            "| Gradient Boosting        | 1.0000       | 0.9561        |\n",
            "\n",
            " Top 5 Feature Importances\n",
            "\n",
            " Top 5 features for [Random Forest]:\n",
            "- worst area               : 0.1539\n",
            "- worst concave points     : 0.1447\n",
            "- mean concave points      : 0.1062\n",
            "- worst radius             : 0.0780\n",
            "- mean concavity           : 0.0680\n",
            "\n",
            " Top 5 features for [Gradient Boosting]:\n",
            "- mean concave points      : 0.4505\n",
            "- worst concave points     : 0.2401\n",
            "- worst radius             : 0.0756\n",
            "- worst perimeter          : 0.0514\n",
            "- worst texture            : 0.0399\n",
            "\n",
            " Final Comparison and Conclusion\n",
            " The model with the best test accuracy is: [Random Forest] (0.9649)\n",
            " The Random Forest and Gradient Boosting models [largely agree] on the most important features\n",
            " They share [3] of the top 5 features, which are typically related to the 'worst' measurements (size/perimeter/area) and concave points\n"
          ]
        }
      ],
      "source": [
        "#Q4:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "print(\"Train and Evaluate Models\")\n",
        "\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "dt_model.fit(X_train, y_train)\n",
        "results['Decision Tree'] = {\n",
        "    'Train Acc': dt_model.score(X_train, y_train),\n",
        "    'Test Acc': dt_model.score(X_test, y_test)\n",
        "}\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf_model.fit(X_train, y_train)\n",
        "results['Random Forest'] = {\n",
        "    'Train Acc': rf_model.score(X_train, y_train),\n",
        "    'Test Acc': rf_model.score(X_test, y_test)\n",
        "}\n",
        "\n",
        "\n",
        "gb_model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "gb_model.fit(X_train, y_train)\n",
        "results['Gradient Boosting'] = {\n",
        "    'Train Acc': gb_model.score(X_train, y_train),\n",
        "    'Test Acc': gb_model.score(X_test, y_test)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\\n Model Performance Comparison:\")\n",
        "print(\"| Model                    | Train Accuracy | Test Accuracy |\")\n",
        "for model_name, accs in results.items():\n",
        "    print(f\"| {model_name.ljust(24)} | {accs['Train Acc']:.4f}       | {accs['Test Acc']:.4f}        |\")\n",
        "\n",
        "print(\"\\n Top 5 Feature Importances\")\n",
        "\n",
        "\n",
        "importances_rf = rf_model.feature_importances_\n",
        "feature_importance_rf = sorted(zip(feature_names, importances_rf), key=itemgetter(1), reverse=True)\n",
        "\n",
        "print(\"\\n Top 5 features for [Random Forest]:\")\n",
        "for name, importance in feature_importance_rf[:5]:\n",
        "    print(f\"- {name.ljust(25)}: {importance:.4f}\")\n",
        "\n",
        "importances_gb = gb_model.feature_importances_\n",
        "feature_importance_gb = sorted(zip(feature_names, importances_gb), key=itemgetter(1), reverse=True)\n",
        "\n",
        "print(\"\\n Top 5 features for [Gradient Boosting]:\")\n",
        "for name, importance in feature_importance_gb[:5]:\n",
        "    print(f\"- {name.ljust(25)}: {importance:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n Final Comparison and Conclusion\")\n",
        "\n",
        "\n",
        "best_test_acc = max(results[m]['Test Acc'] for m in results)\n",
        "best_model = [m for m, accs in results.items() if accs['Test Acc'] == best_test_acc][0]\n",
        "\n",
        "print(f\" The model with the best test accuracy is: [{best_model}] ({best_test_acc:.4f})\")\n",
        "\n",
        "top5_rf_names = [name for name, _ in feature_importance_rf[:5]]\n",
        "top5_gb_names = [name for name, _ in feature_importance_gb[:5]]\n",
        "intersection = set(top5_rf_names).intersection(top5_gb_names)\n",
        "\n",
        "print(f\" The Random Forest and Gradient Boosting models [largely agree] on the most important features\")\n",
        "print(f\" They share [{len(intersection)}] of the top 5 features, which are typically related to the 'worst' measurements (size/perimeter/area) and concave points\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from operator import itemgetter\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "results = {}\n",
        "\n",
        "print(\"Decision Tree (Full and Pruned) & Overfitting Comparison\")\n",
        "\n",
        "dt_full = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "dt_full.fit(X_train, y_train)\n",
        "results['Decision Tree (Full)'] = {\n",
        "    'Train Acc': dt_full.score(X_train, y_train),\n",
        "    'Test Acc': dt_full.score(X_test, y_test)\n",
        "}\n",
        "\n",
        "dt_pruned = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
        "dt_pruned.fit(X_train, y_train)\n",
        "results['Decision Tree (Pruned)'] = {\n",
        "    'Train Acc': dt_pruned.score(X_train, y_train),\n",
        "    'Test Acc': dt_pruned.score(X_test, y_test)\n",
        "}\n",
        "\n",
        "print(\"| Model                    | Train Accuracy | Test Accuracy |\")\n",
        "print(f\"| {'Decision Tree (Full)'.ljust(24)} | {results['Decision Tree (Full)']['Train Acc']:.4f}       | {results['Decision Tree (Full)']['Test Acc']:.4f}        |\")\n",
        "print(f\"| {'Decision Tree (Pruned)'.ljust(24)} | {results['Decision Tree (Pruned)']['Train Acc']:.4f}       | {results['Decision Tree (Pruned)']['Test Acc']:.4f}        |\")\n",
        "\n",
        "print(\"Overfitting Comment:\")\n",
        "print(f\" DT (Full): High overfitting (Train Acc: {results['Decision Tree (Full)']['Train Acc']:.4f} vs. Test Acc: {results['Decision Tree (Full)']['Test Acc']:.4f})\")\n",
        "print(f\" DT (Pruned): Reduced overfitting, better generalization (Train Acc: {results['Decision Tree (Pruned)']['Train Acc']:.4f} vs. Test Acc: {results['Decision Tree (Pruned)']['Test Acc']:.4f})\")\n",
        "\n",
        "print(\"Train Random Forest and Compare with Decision Trees\")\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf_model.fit(X_train, y_train)\n",
        "results['Random Forest (100)'] = {\n",
        "    'Train Acc': rf_model.score(X_train, y_train),\n",
        "    'Test Acc': rf_model.score(X_test, y_test)\n",
        "}\n",
        "\n",
        "print(\"| Model                    | Train Accuracy | Test Accuracy |\")\n",
        "print(f\"| {'Random Forest (100)'.ljust(24)} | {results['Random Forest (100)']['Train Acc']:.4f}       | {results['Random Forest (100)']['Test Acc']:.4f}        |\")\n",
        "print(f\"| {'Decision Tree (Pruned)'.ljust(24)} | {results['Decision Tree (Pruned)']['Train Acc']:.4f}       | {results['Decision Tree (Pruned)']['Test Acc']:.4f}        |\")\n",
        "print(\"Comparison:\")\n",
        "print(f\" Random Forest achieves excellent test accuracy ({results['Random Forest (100)']['Test Acc']:.4f}), outperforming the single, full Decision Tree. This demonstrates the power of Bagging in reducing Variance and Overfitting\")\n",
        "\n",
        "learning_rates = [0.01, 0.1]\n",
        "n_estimators_list = [50, 100, 200]\n",
        "gb_tuning_results = []\n",
        "\n",
        "print(\"| Learning Rate | N_Estimators | Train Accuracy | Test Accuracy |\")\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for n_est in n_estimators_list:\n",
        "        gb_model = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr, random_state=RANDOM_STATE)\n",
        "        gb_model.fit(X_train, y_train)\n",
        "\n",
        "        train_acc = gb_model.score(X_train, y_train)\n",
        "        test_acc = gb_model.score(X_test, y_test)\n",
        "\n",
        "        gb_tuning_results.append({\n",
        "            'lr': lr,\n",
        "            'n_est': n_est,\n",
        "            'Train Acc': train_acc,\n",
        "            'Test Acc': test_acc\n",
        "        })\n",
        "\n",
        "        print(f\"| {lr:.2f}          | {n_est:<12} | {train_acc:.4f}         | {test_acc:.4f}        |\")\n",
        "\n",
        "gb_final = GradientBoostingClassifier(random_state=RANDOM_STATE).fit(X_train, y_train)\n",
        "rf_final = rf_model\n",
        "\n",
        "print(\"Top 5 Feature Importances\")\n",
        "\n",
        "importances_rf = rf_final.feature_importances_\n",
        "feature_importance_rf = sorted(zip(feature_names, importances_rf), key=itemgetter(1), reverse=True)\n",
        "\n",
        "print(\"\\n Top 5 features for Random Forest:\")\n",
        "for name, importance in feature_importance_rf[:5]:\n",
        "    print(f\"- {name.ljust(25)}: {importance:.4f}\")\n",
        "\n",
        "importances_gb = gb_final.feature_importances_\n",
        "feature_importance_gb = sorted(zip(feature_names, importances_gb), key=itemgetter(1), reverse=True)\n",
        "\n",
        "print(\"\\n Top 5 features for Gradient Boosting (Default):\")\n",
        "for name, importance in feature_importance_gb[:5]:\n",
        "    print(f\"- {name.ljust(25)}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS7WkImdSSwp",
        "outputId": "808037e7-9024-49f8-c206-ebd2aa4c6ce2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree (Full and Pruned) & Overfitting Comparison\n",
            "| Model                    | Train Accuracy | Test Accuracy |\n",
            "| Decision Tree (Full)     | 1.0000       | 0.9474        |\n",
            "| Decision Tree (Pruned)   | 0.9780       | 0.9474        |\n",
            "Overfitting Comment:\n",
            " DT (Full): High overfitting (Train Acc: 1.0000 vs. Test Acc: 0.9474)\n",
            " DT (Pruned): Reduced overfitting, better generalization (Train Acc: 0.9780 vs. Test Acc: 0.9474)\n",
            "Train Random Forest and Compare with Decision Trees\n",
            "| Model                    | Train Accuracy | Test Accuracy |\n",
            "| Random Forest (100)      | 1.0000       | 0.9649        |\n",
            "| Decision Tree (Pruned)   | 0.9780       | 0.9474        |\n",
            "Comparison:\n",
            " Random Forest achieves excellent test accuracy (0.9649), outperforming the single, full Decision Tree. This demonstrates the power of Bagging in reducing Variance and Overfitting\n",
            "| Learning Rate | N_Estimators | Train Accuracy | Test Accuracy |\n",
            "| 0.01          | 50           | 0.9780         | 0.9561        |\n",
            "| 0.01          | 100          | 0.9868         | 0.9561        |\n",
            "| 0.01          | 200          | 0.9934         | 0.9561        |\n",
            "| 0.10          | 50           | 1.0000         | 0.9561        |\n",
            "| 0.10          | 100          | 1.0000         | 0.9561        |\n",
            "| 0.10          | 200          | 1.0000         | 0.9561        |\n",
            "Top 5 Feature Importances\n",
            "\n",
            " Top 5 features for Random Forest:\n",
            "- worst area               : 0.1539\n",
            "- worst concave points     : 0.1447\n",
            "- mean concave points      : 0.1062\n",
            "- worst radius             : 0.0780\n",
            "- mean concavity           : 0.0680\n",
            "\n",
            " Top 5 features for Gradient Boosting (Default):\n",
            "- mean concave points      : 0.4505\n",
            "- worst concave points     : 0.2401\n",
            "- worst radius             : 0.0756\n",
            "- worst perimeter          : 0.0514\n",
            "- worst texture            : 0.0399\n"
          ]
        }
      ]
    }
  ]
}